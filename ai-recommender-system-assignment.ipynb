{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11447969,"sourceType":"datasetVersion","datasetId":7172272}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:43:44.639280Z","iopub.execute_input":"2025-04-20T18:43:44.639511Z","iopub.status.idle":"2025-04-20T18:43:46.737940Z","shell.execute_reply.started":"2025-04-20T18:43:44.639491Z","shell.execute_reply":"2025-04-20T18:43:46.737049Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/amazon/test_amazon_ratings.csv\n/kaggle/input/amazon/train_amazon_ratings.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Install\n!pip install scikit-surprise pyspark --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:43:52.817254Z","iopub.execute_input":"2025-04-20T18:43:52.817794Z","iopub.status.idle":"2025-04-20T18:43:57.292958Z","shell.execute_reply.started":"2025-04-20T18:43:52.817768Z","shell.execute_reply":"2025-04-20T18:43:57.291831Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"**Look at the dataset**","metadata":{}},{"cell_type":"code","source":"# ðŸ“‚ Load data\ntrain_df = pd.read_csv(\"/kaggle/input/amazon/train_amazon_ratings.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/amazon/test_amazon_ratings.csv\")\n\n# Prepare sets\ntrain_users = set(train_df[\"UserID\"])\ntrain_items = set(train_df[\"ItemID\"])\ntest_users = set(test_df[\"UserID\"])\ntest_items = set(test_df[\"ItemID\"])\n\n# Overlap analysis\ncommon_users = train_users & test_users\ncommon_items = train_items & test_items\nnew_users = test_users - train_users\nnew_items = test_items - train_items\n\n# Summary statistics\nprint(\"Total train users:\", len(train_users))\nprint(\"Total test users:\", len(test_users))\nprint(\"Overlapping users:\", len(common_users))\nprint(\"New users (cold-start):\", len(new_users))\nprint(\"â€”\" * 40)\nprint(\"Total train items:\", len(train_items))\nprint(\"Total test items:\", len(test_items))\nprint(\"Overlapping items:\", len(common_items))\nprint(\"New items (cold-start):\", len(new_items))\nprint(\"â€”\" * 40)\nprint(f\"Test user overlap rate: {len(common_users)/len(test_users):.2%}\")\nprint(f\"Test item overlap rate: {len(common_items)/len(test_items):.2%}\")\nprint(f\"Estimated fallback usage (user-based): {(1 - len(common_users)/len(test_users)) * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:44:56.071024Z","iopub.execute_input":"2025-04-20T18:44:56.071436Z","iopub.status.idle":"2025-04-20T18:44:56.273299Z","shell.execute_reply.started":"2025-04-20T18:44:56.071412Z","shell.execute_reply":"2025-04-20T18:44:56.272296Z"}},"outputs":[{"name":"stdout","text":"Total train users: 9300\nTotal test users: 8144\nOverlapping users: 8140\nNew users (cold-start): 4\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\nTotal train items: 2970\nTotal test items: 2960\nOverlapping items: 2960\nNew items (cold-start): 0\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\nTest user overlap rate: 99.95%\nTest item overlap rate: 100.00%\nEstimated fallback usage (user-based): 0.05%\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"***Baseline + GridSearch, performance: the best which is 0.882!!!!!!***","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom surprise import Dataset, Reader, BaselineOnly\nfrom surprise.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# Load data\ntrain_df = pd.read_csv(\"/kaggle/input/amazon/train_amazon_ratings.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/amazon/test_amazon_ratings.csv\")\nreader = Reader(rating_scale=(1, 5))\ndata = Dataset.load_from_df(train_df[[\"UserID\", \"ItemID\", \"Rating\"]], reader)\n\n# GridSearchCV for BaselineOnly\nparam_grid = {\n    \"bsl_options\": {\n        \"method\": [\"als\"],\n        \"reg_i\": [5, 10, 15],\n        \"reg_u\": [5, 10, 15]\n    }\n}\ngs = GridSearchCV(BaselineOnly, param_grid, measures=[\"rmse\"], cv=5, n_jobs=-1)\ngs.fit(data)\nbest_params = gs.best_params['rmse']\nprint(\"Best BaselineOnly CV RMSE:\", gs.best_score['rmse'])\nprint(\"Best Params:\", best_params)\n\n# Manual Validation Split\ntrain_split, val_split = train_test_split(train_df, test_size=0.2, random_state=42)\ntrainset = Dataset.load_from_df(train_split[[\"UserID\", \"ItemID\", \"Rating\"]], reader).build_full_trainset()\nmodel = BaselineOnly(**best_params)\nmodel.fit(trainset)\n\n# Fallback Dictionaries\nglobal_mean = train_split[\"Rating\"].mean()\nuser_avg = train_split.groupby(\"UserID\")[\"Rating\"].mean().to_dict()\nitem_avg = train_split.groupby(\"ItemID\")[\"Rating\"].mean().to_dict()\nuser_count = train_split[\"UserID\"].value_counts().to_dict()\nitem_count = train_split[\"ItemID\"].value_counts().to_dict()\nknown_users = set(user_avg)\nknown_items = set(item_avg)\n\ndef fallback(uid, iid):\n    u_avg = user_avg.get(uid)\n    i_avg = item_avg.get(iid)\n    u_c = user_count.get(uid, 0)\n    i_c = item_count.get(iid, 0)\n    if u_avg and i_avg:\n        total = u_c + i_c + 10\n        return (u_c / total) * u_avg + (i_c / total) * i_avg + (10 / total) * global_mean\n    elif u_avg:\n        total = u_c + 10\n        return (u_c / total) * u_avg + (10 / total) * global_mean\n    elif i_avg:\n        total = i_c + 10\n        return (i_c / total) * i_avg + (10 / total) * global_mean\n    else:\n        return global_mean\n\n# Manual Validation RMSE\npreds, truths, fallback_count = [], [], 0\nfor _, row in val_split.iterrows():\n    uid, iid, true = row[\"UserID\"], row[\"ItemID\"], row[\"Rating\"]\n    if uid in known_users and iid in known_items:\n        pred = model.predict(uid, iid).est\n    else:\n        pred = fallback(uid, iid)\n        fallback_count += 1\n    preds.append(pred)\n    truths.append(true)\nmanual_rmse = mean_squared_error(truths, preds, squared=False)\nprint(f\"BaselineOnly Manual RMSE: {manual_rmse:.5f} | Fallback: {fallback_count / len(val_split):.2%}\")\n\n# Final train on full data\nfinal_model = BaselineOnly(**best_params)\nfinal_model.fit(data.build_full_trainset())\n\n# Generate submission\nsubmission = test_df.copy()\nsubmission[\"Prediction\"] = submission.apply(\n    lambda row: final_model.predict(row[\"UserID\"], row[\"ItemID\"]).est\n    if row[\"UserID\"] in known_users and row[\"ItemID\"] in known_items\n    else fallback(row[\"UserID\"], row[\"ItemID\"]), axis=1\n)\nsubmission[[\"id\", \"Prediction\"]].to_csv(\"submission_baselineonly.csv\", index=False)\nprint(\"submission_baselineonly.csv generated!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:45:34.830808Z","iopub.execute_input":"2025-04-20T18:45:34.831083Z","iopub.status.idle":"2025-04-20T18:45:45.867864Z","shell.execute_reply.started":"2025-04-20T18:45:34.831061Z","shell.execute_reply":"2025-04-20T18:45:45.866893Z"}},"outputs":[{"name":"stdout","text":"Best BaselineOnly CV RMSE: 0.9367438489620156\nBest Params: {'bsl_options': {'method': 'als', 'reg_i': 5, 'reg_u': 5}}\nEstimating biases using als...\nBaselineOnly Manual RMSE: 0.94373 | Fallback: 0.27%\nEstimating biases using als...\nsubmission_baselineonly.csv generated!\n","output_type":"stream"}],"execution_count":7}]}